{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Networks using TensorFlow and the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "import load_mnist\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "FLOAT = tf.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Weight(shape, sd=0.02):\n",
    "    initial = tf.random_normal(shape, mean=0, stddev=sd)\n",
    "    return tf.Variable(initial, name=\"weights\")\n",
    "\n",
    "def Bias(shape, sd=0.02):\n",
    "    initial = tf.random_normal(shape, mean=0, stddev=sd)\n",
    "    return tf.Variable(initial, name=\"bias\")\n",
    "\n",
    "def StridedConv(x, W):\n",
    "    return tf.nn.conv2d(\n",
    "        x, W,\n",
    "        strides=[1, 2, 2, 1],\n",
    "        padding=\"SAME\", name=\"strided_conv\")\n",
    "\n",
    "# The function tf.nn.deconv2d is an undecumented part of the TensorFlow API,\n",
    "# and is pretty half baked. For instance, unlike just about every other similar\n",
    "# function, deconv2d can't handle a batch size of -1. Even worse, instead of\n",
    "# throwing a py Exception, it results in an uncaught C++ error that crashes the\n",
    "# iPython kernel. And its signature or internals may unexpectedly change at any\n",
    "# point in the future, since it's not a stable part of the API.\n",
    "#\n",
    "# Anyway, the crux of the matter is: it forces us to commit to a single batch\n",
    "# size for the entire network architecture.\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "def FractionallyStridedConv(x, W, output_channels=None, batch_size=BATCH_SIZE):\n",
    "    _, height, width, kernels = x.get_shape().as_list()\n",
    "    if output_channels is None:\n",
    "        output_channels = kernels / 2\n",
    "    return tf.nn.deconv2d(\n",
    "        x, W,\n",
    "        [batch_size, height * 2, width * 2, output_channels],\n",
    "        [1, 2, 2, 1],\n",
    "        name=\"fractionally_strided_conv\")\n",
    "\n",
    "def ELU(x):\n",
    "    pos = tf.cast(tf.greater_equal(x, 0), FLOAT)\n",
    "    return (pos * x) + ((1 - pos) * (tf.exp(x) - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks\n",
    "See Goodfellow et al (http://arxiv.org/pdf/1406.2661v1.pdf) for a description of GANs.\n",
    "\n",
    "In essence, we pit two network against each other in a game. A discriminative network attempts to determine whether an input image belongs to the training distribution, or is a forgery. A generative network attempts to produce forgeries which fool the discriminative network.\n",
    "\n",
    "See Radford, Metz, & Chintala (http://arxiv.org/pdf/1511.06434v1.pdf) for useful constraints on GAN architecture. The authors make the following recommendations:\n",
    "\n",
    "- Use strided convolutions rather than max pooling.\n",
    "- Use batchnorm everywhere other than generator output and discriminator input.\n",
    "- No dense hidden layers.\n",
    "- ReLU units in the generator (except output, which uses Tanh) and Leaky ReLU in the discriminator.\n",
    "\n",
    "I'll be using exponential liniar units (ELU) instead of ReLU / Leaky ReLU, and see if I can get away without batchnorm, as the ELU addresses some of the same issues: namely, encouraging inputs to each unit to approach 0, and allowing gradients to penetrate further into deep networks. Otherwise, I'll attempt to cleave to their suggestions.\n",
    "\n",
    "The general architecture that Radford et al recommend for the generator is based on DCGAN. A vector of uniformly distributed noise is projected onto a small 2d image with a large number of channels. Repeated \"fractionally strided convolutions\" (sometimes erroneously called \"deconvolutions\") successfively scale down the number of channels and scale up the size of the image.\n",
    "\n",
    "They further recommend the following hyperparameters:\n",
    "- Weights initialized with a 0-centered normal distribution having a std of 0.02\n",
    "- Adam optimizer with a learning rate of 0.0002 and beta-1 of 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del(sess)\n",
    "    print \"deleted session\"\n",
    "except Exception as e:\n",
    "    print \"no existing session to delete\"\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generative Network\n",
    "The generative network maps a vector of uniform random noise inputs to a 28 x 28 1-channel image via a stack of three fractionally strided convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G_nonlin = ELU\n",
    "\n",
    "# Input a stack of 100-dimensional noise vectors.\n",
    "# This spans the underlying object space of 10 digits.\n",
    "G_x = tf.placeholder(FLOAT, [None, 100])\n",
    "\n",
    "# Project noise into a 4x4 image with many channels.\n",
    "# We won't update the weight that controls the projection during training.\n",
    "with tf.name_scope(\"G_projection\") as scope:\n",
    "    G_W_proj = Weight([100, 4 * 4 * 128])\n",
    "    G_b_proj = Bias([4 * 4 * 128])\n",
    "    G_h_proj = tf.reshape(tf.matmul(G_x, G_W_proj) + G_b_proj, [-1, 4, 4, 128])\n",
    "\n",
    "# Apply fractionally strided convolutions to decrease number of channels and increase image size.\n",
    "with tf.name_scope(\"G_fsc1\") as scope:\n",
    "    G_W_fsc1 = Weight([2, 2, 64, 128])\n",
    "    G_b_fsc1 = Bias([64])\n",
    "    G_h_fsc1 = G_nonlin(FractionallyStridedConv(G_h_proj, G_W_fsc1) + G_b_fsc1)\n",
    "\n",
    "with tf.name_scope(\"G_fsc2\") as scope:\n",
    "    G_W_fsc2 = Weight([2, 2, 32, 64])\n",
    "    G_b_fsc2 = Bias([32])\n",
    "    G_h_fsc2 = G_nonlin(FractionallyStridedConv(G_h_fsc1, G_W_fsc2) + G_b_fsc2)\n",
    "\n",
    "with tf.name_scope(\"G_fsc3\") as scope:\n",
    "    G_W_fsc3 = Weight([2, 2, 1, 32])\n",
    "    G_b_fsc3 = Bias([1])\n",
    "    G_h_fsc3 = tf.nn.tanh(FractionallyStridedConv(G_h_fsc2, G_W_fsc3, output_channels=1) + G_b_fsc3)\n",
    "\n",
    "# Trim output to 28x28 and put in the same scale as the source image\n",
    "with tf.name_scope(\"G_out\") as scope:\n",
    "    G_y = tf.slice(G_h_fsc3, [0, 2, 2, 0], [-1, 28, 28, -1])\n",
    "    G_image = tf.reshape((G_y + 1) * 127.5, [-1, 28, 28])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminative Network\n",
    "\n",
    "The discriminative network maps a 28 x 28 1-channel image to single float between 0 and 1, representing the probability that the image came from the training data distribution rather than the generative network.\n",
    "\n",
    "The input of the discriminator is hooked up to the output of the generator. This lets us either train the discriminator, by ignoring the generator portion of the computation graph and providing input images in the feed dict, or the generator, by freezing the variables in the discriminator and feeding in noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D_nonlin = ELU\n",
    "\n",
    "# Input is a stack of 28 x 28 black and white images with activations from 0 to 255.\n",
    "D_x = G_y\n",
    "\n",
    "# Target vector is a 1 (real) or 0 (forgery) for each input.\n",
    "D_y_ = tf.placeholder(FLOAT, [None])\n",
    "\n",
    "# Stack strided convolutional layers.\n",
    "# Input should be 32x32, output 16x16\n",
    "PADDING = [[0, 0], [2, 2], [2, 2], [0, 0]]\n",
    "with tf.name_scope(\"D_sc1\") as scope:\n",
    "    D_W_conv1 = Weight([3, 3, 1, 16])\n",
    "    D_b_conv1 = Bias([16])\n",
    "    D_h_conv1 = D_nonlin(StridedConv(tf.pad(D_x, PADDING), D_W_conv1) + D_b_conv1)\n",
    "\n",
    "# Input should be 16x16, output 8x8\n",
    "with tf.name_scope(\"D_sc2\") as scope:\n",
    "    D_W_conv2 = Weight([3, 3, 16, 32])\n",
    "    D_b_conv2 = Bias([32])\n",
    "    D_h_conv2 = D_nonlin(StridedConv(D_h_conv1, D_W_conv2) + D_b_conv2)\n",
    "\n",
    "# Input should be 8x8, output 4x4\n",
    "with tf.name_scope(\"D_sc3\") as scope:\n",
    "    D_W_conv3 = Weight([3, 3, 32, 64])\n",
    "    D_b_conv3 = Bias([64])\n",
    "    D_h_conv3 = D_nonlin(StridedConv(D_h_conv2, D_W_conv3) + D_b_conv3)\n",
    "\n",
    "# Output a single float between 0 and 1\n",
    "with tf.name_scope(\"D_output\") as scope:\n",
    "    D_h_flat = tf.reshape(D_h_conv3, [-1, 4 * 4 * 64])\n",
    "    D_W_out = Weight([4 * 4 * 64, 1])\n",
    "    D_b_out = Bias([1])\n",
    "    D_y = tf.nn.sigmoid(tf.matmul(D_h_flat, D_W_out) + D_b_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adversarial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generator\n",
    "# When we evaluate the generator training step, we feed noise into G_x.\n",
    "# Never evaluate the generator train step when feeding real images into D_x.\n",
    "\n",
    "G_err = -tf.reduce_mean(tf.log(D_y))\n",
    "G_train_step = tf.train.AdamOptimizer(0.0002, beta1=0.5).minimize(\n",
    "    G_err,\n",
    "    var_list=[\n",
    "        G_W_fsc1, G_b_fsc1,\n",
    "        G_W_fsc2, G_b_fsc2,\n",
    "        G_W_fsc3, G_b_fsc3])\n",
    "\n",
    "# Discriminator\n",
    "# When we evauate the descriminator train step, we should alternate between minibatches\n",
    "# wherein we provide real images fed into D_x and noise fed into G_x.\n",
    "\n",
    "D_xent = -tf.reduce_mean(D_y_ * tf.log(D_y)) - tf.reduce_mean((1 - D_y_) * tf.log(1 - D_y))\n",
    "D_accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.cast(tf.greater_equal(D_y, 0.5), FLOAT), D_y_), FLOAT))\n",
    "D_train_step = tf.train.AdamOptimizer(0.0002, beta1=0.5).minimize(\n",
    "    D_xent,\n",
    "    var_list=[\n",
    "        D_W_conv1, D_b_conv1,\n",
    "        D_W_conv2, D_b_conv2,\n",
    "        D_W_out, D_b_out])\n",
    "\n",
    "# Initialize\n",
    "\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch and read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    (train_im, test_im, train_labels, test_labels)\n",
    "except:\n",
    "    train_im, test_im, train_labels, test_labels = load_mnist.Datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Main loop\n",
    "For each minibatch:\n",
    "1. Train the discriminator on real images.\n",
    "2. Train the discriminator on forgeries.\n",
    "3. Train the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "batch_size = BATCH_SIZE\n",
    "validate_batch_size = BATCH_SIZE\n",
    "validate_every_n_batches = 100\n",
    "\n",
    "figsize(16, 1.5)\n",
    "\n",
    "def PrepBatch(ims, start, stop):\n",
    "    batch = ims[start:stop].reshape((stop - start,) + ims.shape[1:3] + (1,))\n",
    "    return (batch / 127.5) - 1\n",
    "    \n",
    "batches_per_epoch = train_im.shape[0] / batch_size\n",
    "mark = time.time()\n",
    "train_gen = True\n",
    "train_disc = True\n",
    "for ep in xrange(num_epochs):\n",
    "    for i in xrange(batches_per_epoch):\n",
    "        sys.stdout.write(\".\")\n",
    "        sys.stdout.flush()\n",
    "        start_offset = i * batch_size\n",
    "        stop_offset = start_offset + batch_size\n",
    "        \n",
    "        # Train discriminator on real images\n",
    "        D_train_step.run(feed_dict={\n",
    "            D_x: PrepBatch(train_im, start_offset, stop_offset),\n",
    "            D_y_: np.ones(batch_size)})\n",
    "\n",
    "        # Train discriminator on forgeries\n",
    "        D_train_step.run(feed_dict={\n",
    "            G_x: np.random.random((batch_size, 100)),\n",
    "            D_y_: np.zeros(batch_size)})\n",
    "\n",
    "        # Train the generator\n",
    "        G_train_step.run(feed_dict={\n",
    "            G_x: np.random.random((batch_size, 100))})\n",
    "\n",
    "        # Validation\n",
    "        if (i + 1) % validate_every_n_batches == 0:\n",
    "            ac_real, xent_real = sess.run(\n",
    "                [D_accuracy, D_xent], feed_dict={\n",
    "                    D_x: PrepBatch(test_im, 0, validate_batch_size),\n",
    "                    D_y_: np.ones(validate_batch_size)})\n",
    "            ac_forged, xent_forged, im_forged, ger = sess.run(\n",
    "                [D_accuracy, D_xent, G_image, G_err],\n",
    "                feed_dict={\n",
    "                    G_x: np.random.random((validate_batch_size, 100)),\n",
    "                    D_y_: np.zeros(validate_batch_size)})\n",
    "            print (\"\\nEpoch {ep}, batch {ba} ({t:.1f} seconds since last report)\"\n",
    "                   \"\\nDISCRIMINATOR: real {acr:.0f}% / {xr:.5f}, \"\n",
    "                   \"forged {acf:.0f}% / {xf:.5f}\"\n",
    "                   \"\\nGENERATOR: error = {ger:.2f}\").format(\n",
    "                ep=ep, ba=i, t=time.time() - mark,\n",
    "                acr=ac_real * 100, xr=xent_real, acf=ac_forged * 100, xf=xent_forged, ger=ger)\n",
    "            _, axes = plt.subplots(1, 8)\n",
    "            for j in xrange(8):\n",
    "                axes[j].axis(\"off\")\n",
    "                axes[j].imshow(im_forged[j], cmap=cm.Blues)\n",
    "            plt.show()\n",
    "            mark = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.nn.deconv2d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
