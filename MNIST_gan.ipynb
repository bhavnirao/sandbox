{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Networks using TensorFlow and the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "import load_mnist\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch and Read In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_im, test_im, train_labels, test_labels = load_mnist.Datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Weight(shape, init=0.01):\n",
    "    initial = tf.random_uniform(shape, minval=-init, maxval=init)\n",
    "    return tf.Variable(initial, name=\"weights\")\n",
    "\n",
    "def Bias(shape, init=0.01):\n",
    "    initial = tf.constant(init, shape=shape)\n",
    "    return tf.Variable(initial, name=\"bias\")\n",
    "\n",
    "def StridedConv(x, W):\n",
    "    return tf.nn.conv2d(\n",
    "        x, W,\n",
    "        strides=[1, 2, 2, 1],\n",
    "        padding=\"SAME\", name=\"strided_conv\")\n",
    "\n",
    "def FractionallyStridedConv(x, W, output_channels=None):\n",
    "    batchsize, height, width, kernels = x.get_shape().as_list()\n",
    "    if output_channels is None:\n",
    "        output_channels = kernels / 2\n",
    "    return tf.nn.deconv2d(\n",
    "        x, W,\n",
    "        [-1, height * 2, width * 2, output_channels],\n",
    "        [1, 2, 2, 1],\n",
    "        name=\"fractionally_strided_conv\")\n",
    "\n",
    "def ELU(x):\n",
    "    pos = tf.cast(tf.greater_equal(x, 0), tf.float32)\n",
    "    return (pos * x) + ((1 - pos) * (tf.exp(x) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no existing session to delete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   0. ,    0. ,    1. ,   10. ,    2. ,   20. ,    3. ,   30. ],\n",
       "       [   0. ,    0. ,    0.5,    1. ,    1. ,    2. ,    1.5,    3. ],\n",
       "       [   4. ,   40. ,    5. ,   50. ,    6. ,   60. ,    7. ,   70. ],\n",
       "       [   2. ,    4. ,    2.5,    5. ,    3. ,    6. ,    3.5,    7. ],\n",
       "       [   8. ,   80. ,    9. ,   90. ,   10. ,  100. ,   11. ,  110. ],\n",
       "       [   4. ,    8. ,    4.5,    9. ,    5. ,   10. ,    5.5,   11. ],\n",
       "       [  12. ,  120. ,   13. ,  130. ,   14. ,  140. ,   15. ,  150. ],\n",
       "       [   6. ,   12. ,    6.5,   13. ,    7. ,   14. ,    7.5,   15. ]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    del(sess)\n",
    "    print \"deleted session\"\n",
    "except Exception as e:\n",
    "    print \"no existing session to delete\"\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "m = np.arange(16).reshape(1, 4, 4, 1)\n",
    "w = np.array([1, 10, 0.5, 1]).reshape(2, 2, 1, 1)\n",
    "\n",
    "t1 = tf.placeholder(tf.float32, [1, 4, 4, 1])\n",
    "W1 = tf.placeholder(tf.float32, [2, 2, 1, 1])\n",
    "\n",
    "t2 = tf.nn.deconv2d(t1, W1, [1, 8, 8, 1], [1, 2, 2, 1])\n",
    "\n",
    "t2.eval(feed_dict={t1: m, W1: w}).reshape(8, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks\n",
    "See Goodfellow et al (http://arxiv.org/pdf/1406.2661v1.pdf) for a description of GANs.\n",
    "\n",
    "In essence, we pit two network against each other in a game. A discriminative network attempts to determine whether an input image belongs to the training distribution, or is a forgery. A generative network attempts to produce forgeries which fool the discriminative network.\n",
    "\n",
    "See Radford, Metz, & Chintala (http://arxiv.org/pdf/1511.06434v1.pdf) for useful constraints on GAN architecture. The authors make the following recommendations:\n",
    "\n",
    "- Use strided convolutions rather than max pooling.\n",
    "- Use batchnorm everywhere other than generator output and discriminator input.\n",
    "- No dense hidden layers.\n",
    "- ReLU units in the generator (except output, which uses Tanh) and Leaky ReLU in the discriminator.\n",
    "\n",
    "I'll be using exponential liniar units (ELU) instead of ReLU / Leaky ReLU, and see if I can get away without batchnorm, as the ELU addresses some of the same issues: namely, encouraging inputs to each unit to approach 0, and allowing gradients to penetrate further into deep networks. Otherwise, I'll attempt to cleave to their suggestions.\n",
    "\n",
    "The general architecture that Radford et al recommend for the generator is based on DCGAN. A vector of uniformly distributed noise is projected onto a small 2d image with a large number of channels. Repeated \"fractionally strided convolutions\" (sometimes erroneously called \"deconvolutions\") successfively scale down the number of channels and scale up the size of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted session\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del(sess)\n",
    "    print \"deleted session\"\n",
    "except Exception as e:\n",
    "    print \"no existing session to delete\"\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generative Network\n",
    "The generative network maps a vector of uniform random noise inputs to a 28 x 28 1-channel image via a stack of three fractionally strided convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G_nonlin = ELU\n",
    "\n",
    "# Input a stack of 100-dimensional noise vectors.\n",
    "# This spans the underlying object space of 10 digits.\n",
    "G_x = tf.placeholder(tf.float32, [None, 100])\n",
    "\n",
    "# Project noise into a 4x4 image with 256 channels.\n",
    "# We won't update the weight that controls the projection during training.\n",
    "with tf.name_scope(\"G_projection\") as scope:\n",
    "    G_W_proj = Weight([100, 4 * 4 * 256])\n",
    "    G_b_proj = Bias([4 * 4 * 256])\n",
    "    G_h_proj = tf.reshape(tf.matmul(G_x, G_W_proj) + G_b_proj, [-1, 4, 4, 256])\n",
    "\n",
    "# Apply fractionally strided convolutions to decrease number of channels and increase image size.\n",
    "# 4x4 image w/ 256 channels => 8x8 image w/ 128 channels\n",
    "with tf.name_scope(\"G_fsc1\") as scope:\n",
    "    G_W_fsc1 = Weight([2, 2, 128, 256])\n",
    "    G_b_fsc1 = Bias([128])\n",
    "    G_h_fsc1 = G_nonlin(FractionallyStridedConv(G_h_proj, G_W_fsc1) + G_b_fsc1)\n",
    "\n",
    "# 8x8 image w/ 128 channels to 16x16 image with 64 channels\n",
    "with tf.name_scope(\"G_fsc2\") as scope:\n",
    "    G_W_fsc2 = Weight([2, 2, 64, 128])\n",
    "    G_b_fsc2 = Bias([64])\n",
    "    G_h_fsc2 = G_nonlin(FractionallyStridedConv(G_h_fsc1, G_W_fsc2) + G_b_fsc2)\n",
    "\n",
    "# 16x16 image w/ 64 channels to 32x32 image with 1 channel\n",
    "with tf.name_scope(\"G_fsc3\") as scope:\n",
    "    G_W_fsc3 = Weight([2, 2, 1, 64])\n",
    "    G_b_fsc3 = Bias([1])\n",
    "    G_h_fsc3 = tf.nn.tanh(FractionallyStridedConv(G_h_fsc2, G_W_fsc3, output_channels=1) + G_b_fsc3)\n",
    "\n",
    "# Pretty output in the same scale as images\n",
    "with tf.name_scope(\"G_out\") as scope:\n",
    "    G_y = tf.slice(G_h_fsc3, [0, 2, 2, 0], [-1, 28, 28, -1])\n",
    "    G_image = (G_y + 1) * 127.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminative Network\n",
    "\n",
    "The discriminative network maps a 28 x 28 1-channel image to single float between 0 and 1, representing the probability that the image came from the training data distribution rather than the generative network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D_nonlin = ELU\n",
    "\n",
    "# Input is a stack of 28 x 28 black and white images with activations from 0 to 255.\n",
    "D_x = G_y\n",
    "\n",
    "# Target vector is a 1 (real) or 0 (forgery) for each input.\n",
    "D_y_ = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "# Stack strided convolutional layers.\n",
    "# Input should be 28x28, output 14x14\n",
    "with tf.name_scope(\"D_sc1\") as scope:\n",
    "    D_W_conv1 = Weight([3, 3, 1, 16])\n",
    "    D_b_conv1 = Bias([16])\n",
    "    D_h_conv1 = D_nonlin(StridedConv(D_x, D_W_conv1) + D_b_conv1)\n",
    "\n",
    "# Input should be 14x14, output 7x7\n",
    "with tf.name_scope(\"D_sc2\") as scope:\n",
    "    D_W_conv2 = Weight([3, 3, 16, 32])\n",
    "    D_b_conv2 = Bias([32])\n",
    "    D_h_conv2 = D_nonlin(StridedConv(D_h_conv1, D_W_conv2) + D_b_conv2)\n",
    "\n",
    "# Output a single float between 0 and 1\n",
    "with tf.name_scope(\"D_output\") as scope:\n",
    "    D_h_flat = tf.reshape(D_h_conv2, [-1, 7 * 7 * 32])\n",
    "    D_W_out = Weight([7 * 7 * 32, 1])\n",
    "    D_b_out = Bias([1])\n",
    "    D_y = tf.nn.sigmoid(tf.matmul(D_h_flat, D_W_out) + D_b_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adversarial Training\n",
    "0. Start with a batch of noise vectors.\n",
    "1. Feed the noise vectors to the generator to create a batch of forgeries.\n",
    "2. Mix in with a batch of real training images.\n",
    "3. Train the discriminator on the mixed bag.\n",
    "4. Train the generator on the noise vectors from step 0, using the output of the discriminator as the error signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generator\n",
    "# When we evaluate the generator training step, we feed noise into G_x.\n",
    "# Never evaluate the generator train step when feeding real images into D_x.\n",
    "\n",
    "G_err = -tf.reduce_mean(tf.log(D_y))\n",
    "G_train_step = tf.train.AdamOptimizer(0.001).minimize(\n",
    "    G_err,\n",
    "    var_list=[\n",
    "        G_W_fsc1, G_b_fsc1,\n",
    "        G_W_fsc2, G_b_fsc2,\n",
    "        G_W_fsc3, G_b_fsc3])\n",
    "\n",
    "# Discriminator\n",
    "# When we evauate the descriminator train step, we should alternate between minibatches\n",
    "# wherein we provide real images fed into D_x and noise fed into G_x.\n",
    "\n",
    "D_xent = -tf.reduce_mean(D_y_ * tf.log(D_y)) - tf.reduce_mean((1 - D_y_) * tf.log(1 - D_y))\n",
    "D_accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.cast(tf.greater_equal(D_y, 0.5), tf.float32), D_y_), tf.float32))\n",
    "D_train_step = tf.train.AdamOptimizer(0.00001).minimize(\n",
    "    D_xent,\n",
    "    var_list=[\n",
    "        D_W_conv1, D_b_conv1,\n",
    "        D_W_conv2, D_b_conv2,\n",
    "        D_W_out, D_b_out])\n",
    "\n",
    "# Initialize\n",
    "\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "test_every_n_batches = 30\n",
    "\n",
    "figsize(6, 1.5)\n",
    "\n",
    "def PrepBatch(ims, start, stop):\n",
    "    batch = ims[start:stop].reshape((stop - start,) + ims.shape[1:3] + (1,))\n",
    "    return (batch / 127.5) - 1\n",
    "    \n",
    "\n",
    "batches_per_epoch = train_im.shape[0] / batch_size\n",
    "mark = time.time()\n",
    "train_gen = True\n",
    "train_disc = True\n",
    "for ep in xrange(num_epochs):\n",
    "    for i in xrange(batches_per_epoch):\n",
    "        sys.stdout.write(\".\")\n",
    "        sys.stdout.flush()\n",
    "        start_offset = i * batch_size\n",
    "        stop_offset = start_offset + batch_size\n",
    "        \n",
    "        # Train discriminator\n",
    "        if train_disc:\n",
    "            # ...on real images\n",
    "            D_train_step.run(feed_dict={\n",
    "                D_x: PrepBatch(train_im, start_offset, stop_offset),\n",
    "                D_y_: np.ones(batch_size)})\n",
    "\n",
    "            # ...on forgeries\n",
    "            D_train_step.run(feed_dict={\n",
    "                G_x: np.random.random((batch_size, 100)),\n",
    "                D_y_: np.zeros(batch_size)})\n",
    "\n",
    "        # Train the generator\n",
    "        if train_gen:\n",
    "            G_train_step.run(feed_dict={\n",
    "                G_x: np.random.random((batch_size, 32))})\n",
    "\n",
    "        if (i + 1) % test_every_n_batches == 0:\n",
    "            ac_real, xent_real = sess.run(\n",
    "                [D_accuracy, D_xent], feed_dict={\n",
    "                    D_x: PrepBatch(test_im, 0, 100),\n",
    "                    D_y_: np.ones(100)})\n",
    "            ac_forged, xent_forged, im_forged, ger = sess.run(\n",
    "                [D_accuracy, D_xent, G_image, G_err],\n",
    "                feed_dict={\n",
    "                    G_x: np.random.random((100, 100)),\n",
    "                    D_y_: np.zeros(100)})\n",
    "            print (\"\\nEpoch {ep}, batch {ba} ({t:.1f} seconds since last report)\"\n",
    "                   \"\\nDISCRIMINATOR: real {acr:.2f}% / {xr:.5f}, \"\n",
    "                   \"forged accuracy {acf:.1f}% / {xf:.5f}\"\n",
    "                   \"\\nGENERATOR: error = {ger:.2f}\").format(\n",
    "                ep=ep, ba=i, t=time.time() - mark,\n",
    "                acr=ac_real * 100, xr=xent_real, acf=ac_forged * 100, xf=xent_forged, ger=ger)\n",
    "            _, axes = plt.subplots(1, 3)\n",
    "            for j in xrange(3):\n",
    "                axes[j].imshow(im_forged[j], cmap=cm.Blues)\n",
    "            plt.show()\n",
    "            mark = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
