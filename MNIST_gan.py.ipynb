{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Networks using TensorFlow and the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "import load_mnist\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "figsize(4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch and Read In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_im, test_im, train_labels, test_labels = load_mnist.Datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape, init=0.01):\n",
    "    initial = tf.random_uniform(shape, minval=-init, maxval=init)\n",
    "    return tf.Variable(initial, name=\"weights\")\n",
    "\n",
    "def bias_variable(shape, init=0.01):\n",
    "    initial = tf.constant(init, shape=shape)\n",
    "    return tf.Variable(initial, name=\"bias\")\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME', name=\"conv\")\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(\n",
    "        x, ksize=[1, 2, 2, 1],\n",
    "        strides=[1, 2, 2, 1], padding='SAME', name=\"max_pool\")\n",
    "\n",
    "def elu(x):\n",
    "    pos = tf.cast(tf.greater_equal(x, 0), tf.float32)\n",
    "    return (pos * x) + ((1 - pos) * (tf.exp(x) - 1))\n",
    "\n",
    "def HiddenLayer(inp, shape, nonlin=elu, init=0.001, scope=\"RBM\"):\n",
    "    with tf.name_scope(scope) as ns:\n",
    "        W = weight_variable(shape, init=init)\n",
    "        b = bias_variable([shape[1]], init=init)\n",
    "        h = nonlin(tf.matmul(inp, W) + b)\n",
    "        return W, b, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks\n",
    "See http://arxiv.org/pdf/1406.2661v1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted session\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del(sess)\n",
    "    print \"deleted session\"\n",
    "except Exception as e:\n",
    "    print \"no existing session to delete\"\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generative Network\n",
    "The generative network maps a vector of uniform random noise inputs to a 28 x 28 1-channel image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input a stack of 32-dimensional noise vectors.\n",
    "# This spans the underlying object space of 10 digits.\n",
    "G_x = tf.placeholder(tf.float32, [None, 32])\n",
    "\n",
    "# Target vector is the probability of being real or a forgery assigned by the discriminator.\n",
    "G_y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# Scale up input from 10 units to 28 * 28 = 784 units\n",
    "G_W_s1, G_b_s1, G_h_s1 = HiddenLayer(G_x, [32, 128], scope=\"G_scaleup1\")\n",
    "G_W_s2, G_b_s2, G_h_s2 = HiddenLayer(G_h_s1, [128, 512], scope=\"G_scaleup2\")\n",
    "\n",
    "# Apply dropout before last scaleup\n",
    "with tf.name_scope(\"G_dropout\") as scope:\n",
    "    G_keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "    G_h_dropout = tf.nn.dropout(G_h_s2, G_keep_prob)\n",
    "\n",
    "# Scale up to final size, reshape to image dimensions\n",
    "G_W_final, G_b_final, G_h_final = HiddenLayer(\n",
    "    G_h_dropout, [512, 28 * 28], scope=\"G_scaleup_final\")\n",
    "G_out = tf.nn.sigmoid(tf.reshape(G_h_final, [-1, 28, 28]))\n",
    "G_out_image = 255 * G_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminative Network\n",
    "\n",
    "The discriminative network maps a 28 x 28 1-channel image to single float between 0 and 1, representing the probability that the image came from the training data distribution rather than the generative network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input is a stack of 28 x 28 black and white images with activations from 0 to 255.\n",
    "D_x = tf.placeholder(tf.float32, [None, 28, 28])\n",
    "\n",
    "# Target vector is a 1 (real) or 0 (forgery) for each input.\n",
    "D_y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# Stack a couple of convolutional layers\n",
    "nonlin = elu\n",
    "\n",
    "with tf.name_scope(\"D_conv1\") as scope:\n",
    "    D_x_image = tf.reshape(D_x, [-1, 28, 28, 1])\n",
    "    D_W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "    D_b_conv1 = bias_variable([32])\n",
    "    D_h_conv1 = nonlin(conv2d(D_x_image, D_W_conv1) + D_b_conv1)\n",
    "    D_h_pool1 = max_pool_2x2(D_h_conv1)\n",
    "\n",
    "with tf.name_scope(\"D_conv2\") as scope:\n",
    "    D_W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "    D_b_conv2 = bias_variable([64])\n",
    "    D_h_conv2 = nonlin(conv2d(D_h_pool1, D_W_conv2) + D_b_conv2)\n",
    "    D_h_pool2 = max_pool_2x2(D_h_conv2)\n",
    "\n",
    "# Hidden layer and dropout\n",
    "with tf.name_scope(\"D_dense\") as scope:\n",
    "    D_W_dense = weight_variable([7 * 7 * 64, 256])\n",
    "    D_b_dense = bias_variable([256])\n",
    "    D_h_dense = nonlin(tf.matmul(tf.reshape(D_h_pool2, [-1, 7 * 7 * 64]), D_W_dense) + D_b_dense)\n",
    "\n",
    "with tf.name_scope(\"D_dropout\") as scope:\n",
    "    D_keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "    D_h_dropout = tf.nn.dropout(D_h_dense, D_keep_prob)\n",
    "\n",
    "# Output a single float between 0 and 1\n",
    "with tf.name_scope(\"D_output\") as scope:\n",
    "    D_W_out = weight_variable([256, 1])\n",
    "    D_b_out = bias_variable([1])\n",
    "    D_y = tf.nn.sigmoid(tf.matmul(D_h_dropout, D_W_out) + D_b_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adversarial Training\n",
    "0. Start with a batch of noise vectors.\n",
    "1. Feed the noise vectors to the generator to create a batch of forgeries.\n",
    "2. Mix in with a batch of real training images.\n",
    "3. Train the discriminator on the mixed bag.\n",
    "4. Train the generator on the noise vectors from step 0, using the output of the discriminator as the error signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-18f909e9de7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mG_W_s1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_b_s1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mG_W_s2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_b_s2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         G_W_final, G_b_final])\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jeffreyschecter/anaconda/lib/python2.7/site-packages/tensorflow/python/training/optimizer.pyc\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, name)\u001b[0m\n\u001b[1;32m    165\u001b[0m                                             gate_gradients=gate_gradients)\n\u001b[1;32m    166\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n\u001b[0;32m--> 167\u001b[0;31m                                 name=name)\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgate_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGATE_OP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jeffreyschecter/anaconda/lib/python2.7/site-packages/tensorflow/python/training/optimizer.pyc\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[1;32m    243\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_valid_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m     \u001b[0mupdate_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jeffreyschecter/anaconda/lib/python2.7/site-packages/tensorflow/python/training/adam.pyc\u001b[0m in \u001b[0;36m_create_slots\u001b[0;34m(self, var_list)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# variable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_beta1_power\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_beta1_power\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_beta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"beta1_power\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_beta2_power\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_beta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"beta2_power\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Generator\n",
    "\n",
    "G_err = -tf.reduce_mean(tf.log(G_y_))\n",
    "G_train_step = tf.train.AdamOptimizer(0.0001).minimize(\n",
    "    G_err,\n",
    "    var_list=[\n",
    "        G_W_s1, G_b_s1,\n",
    "        G_W_s2, G_b_s2,\n",
    "        G_W_final, G_b_final])\n",
    "\n",
    "# Discriminator\n",
    "\n",
    "D_xent = -tf.reduce_mean(D_y_ * tf.log(D_y)) - tf.reduce_mean((1 - D_y_) * tf.log(1 - D_y))\n",
    "D_accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.cast(tf.greater_equal(D_y, 0.5), tf.float32), D_y_), tf.float32))\n",
    "D_train_step = tf.train.AdamOptimizer(0.0001).minimize(\n",
    "    D_xent,\n",
    "    var_list=[\n",
    "        D_W_conv1, D_b_conv1,\n",
    "        D_W_conv2, D_b_conv2,\n",
    "        D_W_dense, D_b_dense,\n",
    "        D_W_out, D_b_out])\n",
    "\n",
    "# Initialize\n",
    "\n",
    "sess.run(tf.initialize_all_variables())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf.train.AdamOptimizer(0.0001).compute_gradients(G_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
