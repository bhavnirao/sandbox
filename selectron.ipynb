{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "from IPython.core.pylabtools import figsize\n",
    "from seaborn import plt\n",
    "\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def t_softmax(x, temp):\n",
    "    exps = tf.exp(x / temp)\n",
    "    return exps / tf.reduce_sum(exps, axis=1, keep_dims=True)\n",
    "\n",
    "\n",
    "def t_squaremax(x, temp):\n",
    "    sq = tf.square((x + temp) / temp)\n",
    "    return sq / tf.reduce_sum(sq, axis=1, keep_dims=True)\n",
    "\n",
    "\n",
    "def min_max_scale(x, k=1.0):\n",
    "    mins = tf.reduce_min(x, axis=1, keep_dims=True)\n",
    "    scales = tf.reduce_max(x, axis=1, keep_dims=True) - mins\n",
    "    return ((x - mins) / scales) * k\n",
    "\n",
    "\n",
    "def leaky_relu(leak=0.03):\n",
    "    def lrelu(x):\n",
    "        pos = tf.cast(x >= 0, tf.float32)\n",
    "        return (pos * x) + ((1 - pos) * x * leak)\n",
    "    return lrelu\n",
    "\n",
    "\n",
    "class ReuseableConv1D(object):\n",
    "    \n",
    "    def __init__(self, in_ch, out_ch, act=None):\n",
    "        self._act = act\n",
    "        with tf.variable_scope(\"ReuseableConv1D\"):\n",
    "            w_init = tf.random_normal([1, in_ch, out_ch], mean=0, stddev=1.0 / in_ch)\n",
    "            self.w = tf.Variable(w_init, name=\"Weight\")\n",
    "            self.b = tf.Variable(tf.zeros([out_ch]), name=\"Bias\")\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        with tf.variable_scope(\"ReuseableConv1D\"):\n",
    "            h = tf.nn.conv1d(x, self.w, 1, \"VALID\") + self.b\n",
    "            if self._act is None:\n",
    "                return h\n",
    "            else:\n",
    "                return self._act(h)\n",
    "\n",
    "\n",
    "def reuseable_convs(pattern, name, final=None):\n",
    "    with tf.variable_scope(name):\n",
    "        return [\n",
    "            ReuseableConv1D(\n",
    "                pattern[i], pattern[i + 1],\n",
    "                act=final if i == len(pattern) - 2 else leaky_relu())\n",
    "            for i in np.arange(len(pattern) - 1)]\n",
    "    \n",
    "\n",
    "class BasketGenerator(object):\n",
    "    \n",
    "    def __init__(\n",
    "            self, sess=None, name=None, n_choices=3, n_candidates=50,\n",
    "            softmax_fn=t_squaremax, candidate_ch=32, context_ch=32,\n",
    "            selector_hidden_ch=[32, 16], context_hidden_ch=[32, 32],\n",
    "            context_encoder_ch=[32, 32, 32], verbose=1):\n",
    "\n",
    "        # Parameters\n",
    "        self._n_choices = n_choices\n",
    "        self._n_cnd = n_candidates\n",
    "        self._softmax = softmax_fn\n",
    "        self._cnd_ch = candidate_ch\n",
    "        self._cxt_ch = context_ch\n",
    "        self._cxt_enc_ch_pattern = context_encoder_ch\n",
    "        self._cxt_enc_ch = context_encoder_ch[-1]\n",
    "        self._total_ch = candidate_ch + self._cxt_enc_ch\n",
    "        self._sel_ch_pattern = [self._total_ch] + selector_hidden_ch + [1]\n",
    "        self._cxt_ch_pattern = [self._total_ch] + context_hidden_ch + [self._cxt_enc_ch]\n",
    "        self.verbose = 1\n",
    "        \n",
    "        # Netork Initialization\n",
    "        self._sess = sess or tf.get_default_session() or tf.InteractiveSession()\n",
    "        self._name = \"BasketNet_{}\".format(name or np.random.randint(0, 1000000))\n",
    "        self._build()\n",
    "        self._sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Book keeping\n",
    "        self._epochs = 0\n",
    "        self._batches = 0\n",
    "        self._examples = [0]\n",
    "        self._lrs = []\n",
    "        self._train_losses = []\n",
    "        self._temperatures = []\n",
    "        self._learn_rates = []\n",
    "        self._valid_examples = []\n",
    "        self._valid_losses = []\n",
    "    \n",
    "    def fit(\n",
    "            self, train_cnd_cxt_targets, val_cnd_cxt_targets,\n",
    "            batch_size=64, epochs=1, temp_halflife=1000 * 1000, lr=0.002,\n",
    "            report_every=100, validate_at_start=False):\n",
    "        if validate_at_start:\n",
    "            self._validate(\n",
    "                *val_cnd_cxt_targets,\n",
    "                batch_size=batch_size, report_every=report_every)\n",
    "        for ep in np.arange(epochs):\n",
    "            ep_lr = lr[ep] if isinstance(lr, (list, tuple)) else lr\n",
    "            self._train_epoch(\n",
    "                *train_cnd_cxt_targets,\n",
    "                batch_size=batch_size, temp_halflife=temp_halflife,\n",
    "                lr=ep_lr, report_every=report_every)\n",
    "            self._validate(\n",
    "                *val_cnd_cxt_targets,\n",
    "                batch_size=batch_size, report_every=report_every)\n",
    "        self._report()\n",
    "    \n",
    "    def _train_batch(self, cnd, cxt, targets, temp=1.0, lr=0.001):\n",
    "        _, loss = self._sess.run(\n",
    "            [self._train, self._loss],\n",
    "            feed_dict={\n",
    "                self._inp_cnd: cnd,\n",
    "                self._inp_cxt: cxt,\n",
    "                self._targets: targets,\n",
    "                self._temperature: temp,\n",
    "                self._lr: lr})\n",
    "        self._examples.append(self._examples[-1] + cnd.shape[0])\n",
    "        self._train_losses.append(loss)\n",
    "        self._temperatures.append(temp)\n",
    "        self._learn_rates.append(lr)\n",
    "        self._batches += 1\n",
    "    \n",
    "    def _train_epoch(\n",
    "            self, cnd, cxt, targets,\n",
    "            batch_size=64, temp_halflife=1000 * 1000, lr=0.001, report_every=100):\n",
    "        ixs = np.arange(cnd.shape[0])\n",
    "        np.random.shuffle(ixs)\n",
    "        batch_slice_starts = np.arange(0, cnd.shape[0], batch_size)\n",
    "        for i, slice_start in enumerate(batch_slice_starts):\n",
    "            batch_ixs = ixs[slice_start:slice_start + batch_size]\n",
    "            xmpl = float(self._examples[-1])\n",
    "            temp = 0.05 + (0.95 * (0.5 ** (xmpl / temp_halflife)))\n",
    "            self._lrs.append(lr(self._examples[-1]) if callable(lr) else lr)\n",
    "            self._train_batch(\n",
    "                cnd[batch_ixs],\n",
    "                cxt[batch_ixs],\n",
    "                targets[batch_ixs],\n",
    "                temp=temp,\n",
    "                lr=self._lrs[-1])\n",
    "            if report_every is not None and (i + 1) % report_every == 0:\n",
    "                self._report(progress=float(slice_start) / cnd.shape[0])\n",
    "            if not np.isfinite(self._train_losses[-1]):\n",
    "                print \"Encountered infinite loss!\"\n",
    "                return\n",
    "        self._epochs += 1\n",
    "\n",
    "    def _report(self, progress=0.0):\n",
    "        if self.verbose == 0:\n",
    "            return\n",
    "        clear_output(wait=True)\n",
    "        reports = (\n",
    "            (\"Epoch\", self._epochs),\n",
    "            (\"Batch\", self._batches),\n",
    "            (\"Epoch Progress\", \"{:.1f}%\".format(progress * 100)),\n",
    "            (\"Learning Rate\", self._lrs[-1]),\n",
    "            (\"Examples Seen\", self._examples[-1]),\n",
    "            (\"Temperature\", self._temperatures[-1]),\n",
    "            (\"Train Loss (last batch)\", self._train_losses[-1]),\n",
    "            (\"Train Loss (avg last 100 batches)\",\n",
    "             np.mean(self._train_losses[-100:])),\n",
    "            (\"Last Validation Loss\",\n",
    "             self._valid_losses[-1] if self._valid_losses else None))\n",
    "        just = max(len(l) for l, _ in reports)\n",
    "        for label, value in reports:\n",
    "            print \"{}: {}\".format(label.rjust(just), value)\n",
    "\n",
    "    def _validate(self, cnd, cxt, targets, batch_size=64, report_every=100):\n",
    "        batch_starts = np.arange(0, cnd.shape[0], batch_size)\n",
    "        losses = []\n",
    "        for i, start in enumerate(batch_starts):\n",
    "            end = start + batch_size\n",
    "            losses.append(self._sess.run([self._loss], feed_dict={\n",
    "                self._inp_cnd: cnd[start:end],\n",
    "                self._inp_cxt: cxt[start:end],\n",
    "                self._targets: targets[start:end],\n",
    "                self._temperature: 0.05})[0])\n",
    "            if report_every is not None and (i + 1) % report_every == 0:\n",
    "                self._validation_report(start, cnd.shape[0])\n",
    "        self._valid_examples.append(self._examples[-1])\n",
    "        self._valid_losses.append(np.mean(losses))\n",
    "            \n",
    "    def _validation_report(self, so_far, out_of):\n",
    "        if self.verbose == 0:\n",
    "            return\n",
    "        clear_output(wait=True)\n",
    "        print \"Validating, {}/{}\".format(so_far, out_of)\n",
    "    \n",
    "    def _build(self):\n",
    "        with tf.variable_scope(self._name):\n",
    "            self._build_inputs()\n",
    "            self._build_context_encoder()\n",
    "            self._build_selector()\n",
    "            self._build_loss()\n",
    "    \n",
    "    def _build_inputs(self):\n",
    "        with tf.variable_scope(\"Inputs\"):\n",
    "            self._inp_cnd = tf.placeholder(\n",
    "                tf.float32, shape=[None, self._n_cnd, self._cnd_ch],\n",
    "                name=\"CandidatesInput\")\n",
    "            self._inp_cxt = tf.placeholder(\n",
    "                tf.float32, shape=[None, self._cxt_ch],\n",
    "                name=\"ContextInput\")\n",
    "            self._temperature = tf.placeholder(\n",
    "                tf.float32, shape=[],\n",
    "                name=\"Temperature\")\n",
    "    \n",
    "    def _build_context_encoder(self):\n",
    "        with tf.variable_scope(\"InitialContextEncoder\"):\n",
    "            next_input = self._inp_cxt\n",
    "            for ch in self._cxt_enc_ch_pattern:\n",
    "                next_input = tf.contrib.layers.fully_connected(\n",
    "                    next_input, ch, leaky_relu())\n",
    "        self._encoded_initial_cxt = tf.expand_dims(\n",
    "            next_input, axis=1, name=\"EncodedContext\")\n",
    "    \n",
    "    def _selector_block(self):\n",
    "        with tf.variable_scope(\"SelectorBlock\"):\n",
    "            # Concatenate a copy of the context vector to each candidate\n",
    "            cxt = self._cxts[-1]\n",
    "            cxt_tiled = tf.tile(cxt, [1, self._n_cnd, 1])\n",
    "            sel_hidden = tf.concat(2, [self._inp_cnd, cxt_tiled])\n",
    "            \n",
    "            # Apply a non-linear mapping to each candidate to get a fitness score\n",
    "            for conv in self._sel_convs:\n",
    "                sel_hidden = conv(sel_hidden)\n",
    "            \n",
    "            # Select the candidate with the highest fitness\n",
    "            eligible = self._eligibilities[-1]\n",
    "            raw_fitness = eligible * min_max_scale(tf.squeeze(sel_hidden, axis=2), 1.0)\n",
    "            fitness = self._softmax(raw_fitness, self._temperature)\n",
    "            selected_ix = tf.argmax(fitness, axis=1)\n",
    "            self._selection_ixs.append(selected_ix)\n",
    "\n",
    "            # And get the vector representation of that candidate.\n",
    "            # To keep it differentiable, instead of using a binary mask, we use\n",
    "            # a softmax with a temperature parameter that is gradually annealed\n",
    "            # during training.\n",
    "            soft_mask = tf.tile(\n",
    "                tf.expand_dims(fitness, -1),\n",
    "                [1, 1, self._cnd_ch])\n",
    "            selected_repr = tf.reduce_mean(\n",
    "                soft_mask * self._inp_cnd, axis=1, keep_dims=True)\n",
    "\n",
    "            self._basket_items.append(selected_repr)\n",
    "            \n",
    "            # Then mark that candidate as ineligible for future selection\n",
    "            ineligible = tf.one_hot(selected_ix, self._n_cnd)\n",
    "            self._eligibilities.append(eligible - ineligible)\n",
    "            \n",
    "            # Create the new context vector\n",
    "            cxt_hidden = tf.concat(2, [selected_repr, cxt])\n",
    "            for conv in self._cxt_convs:\n",
    "                cxt_hidden = conv(cxt_hidden)\n",
    "            self._cxts.append(cxt + cxt_hidden)\n",
    "    \n",
    "    def _build_selector(self):\n",
    "        with tf.variable_scope(\"Selector\"):\n",
    "            self._basket_items = []\n",
    "            self._selection_ixs = []\n",
    "            self._cxts = [self._encoded_initial_cxt]\n",
    "            self._eligibilities = [\n",
    "                tf.ones_like(tf.reduce_max(self._inp_cnd, axis=2), name=\"Eligible\")]\n",
    "            self._sel_convs = reuseable_convs(\n",
    "                self._sel_ch_pattern, \"Selector\", final=None)\n",
    "            self._cxt_convs = reuseable_convs(\n",
    "                self._cxt_ch_pattern, \"Context\", final=tf.nn.tanh)\n",
    "            for _ in np.arange(self._n_choices):\n",
    "                self._selector_block()\n",
    "            self._basket = tf.concat(1, self._basket_items)\n",
    "            self._basket_labels = tf.concat(\n",
    "                1, [tf.expand_dims(x, axis=1) for x in self._selection_ixs])\n",
    "    \n",
    "    def _build_loss(self):\n",
    "        with tf.variable_scope(\"Loss\"):\n",
    "            self._targets = tf.placeholder(\n",
    "                tf.float32, shape=[None, self._n_choices, self._cnd_ch],\n",
    "                name=\"Targets\")\n",
    "            best_cos_similarities = tf.reduce_max(\n",
    "                tf.matmul(\n",
    "                    tf.nn.l2_normalize(self._targets, 2),\n",
    "                    tf.nn.l2_normalize(self._basket, 2),\n",
    "                    transpose_b=True),\n",
    "                axis=2)\n",
    "            self._loss = tf.reduce_mean(1 - best_cos_similarities)            \n",
    "            self._lr = tf.placeholder(tf.float32, [])\n",
    "            self._train = tf.train.AdamOptimizer(\n",
    "                learning_rate=self._lr).minimize(self._loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
